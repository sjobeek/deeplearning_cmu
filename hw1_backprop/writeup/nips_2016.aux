\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Overfitting and Model Complexity}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Typical Training Behavior: Error Rate vs Model Complexity}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Typical Training Behavior: Error Rate vs Training Set Size}{1}{subsection.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Expected Loss for Regression}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}a) Case q = 1}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}b) Case q -> 0}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Binary Classification Error Function}{2}{section.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Generalized Gaussian}{2}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Show Distribution is Normalized}{2}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Derive Log-likelihood Function}{3}{subsection.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Implementation of Backpropagation}{3}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}a) Basic Generalization}{3}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Training cross-entropy error (blue) and test cross-entropy error (red) for 5 seeds.}}{3}{figure.1}}
\newlabel{fig:crosen1}{{1}{3}{Training cross-entropy error (blue) and test cross-entropy error (red) for 5 seeds}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}b) Classification Error}{4}{subsection.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training classification error (blue) and test classification error (red) for 5 seeds.}}{4}{figure.2}}
\newlabel{fig:class1}{{2}{4}{Training classification error (blue) and test classification error (red) for 5 seeds}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}c) Visualizing Parameters}{4}{subsection.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}d) Learning Rate}{4}{subsection.5.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Visualization of 10-by-10 grid of learned parameters from the bottom layer of the network.}}{5}{figure.3}}
\newlabel{fig:paramviz}{{3}{5}{Visualization of 10-by-10 grid of learned parameters from the bottom layer of the network}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Visualization of error rates with various learning rates. Red and blue are cross-entropy error for test and train, while green and black are categorization error for test and train respectively. }}{5}{figure.4}}
\newlabel{fig:learnrates}{{4}{5}{Visualization of error rates with various learning rates. Red and blue are cross-entropy error for test and train, while green and black are categorization error for test and train respectively}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}e) Number of Hidden Units}{5}{subsection.5.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Visualization of error rates with various momentum values. Red and blue are cross-entropy error for test and train, while green and black are categorization error for test and train respectively. }}{6}{figure.5}}
\newlabel{fig:momentums}{{5}{6}{Visualization of error rates with various momentum values. Red and blue are cross-entropy error for test and train, while green and black are categorization error for test and train respectively}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Visualization of error rates with various numbers of hidden layers. Red and blue are cross-entropy error for test and train, while green and black are categorization error for test and train respectively. }}{6}{figure.6}}
\newlabel{fig:hiddenunits}{{6}{6}{Visualization of error rates with various numbers of hidden layers. Red and blue are cross-entropy error for test and train, while green and black are categorization error for test and train respectively}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}f) Dropout}{6}{subsection.5.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}g) Best-Performing Single Layer}{6}{subsection.5.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}h) Extension to Multiple Layers}{6}{subsection.5.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Visualization of error rates of the single-layer network with the best performance. Red and blue are cross-entropy error for test and train, while green and black are categorization error for test and train respectively. }}{7}{figure.7}}
\newlabel{fig:momentumsrate1}{{7}{7}{Visualization of error rates of the single-layer network with the best performance. Red and blue are cross-entropy error for test and train, while green and black are categorization error for test and train respectively}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Visualization of error rates with various momentum values. Red and blue are cross-entropy error for test and train, while green and black are categorization error for test and train respectively. }}{7}{figure.8}}
\newlabel{fig:momentumsrate1}{{8}{7}{Visualization of error rates with various momentum values. Red and blue are cross-entropy error for test and train, while green and black are categorization error for test and train respectively}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Visualization of error rates with various momentum values. Red and blue are cross-entropy error for test and train, while green and black are categorization error for test and train respectively. }}{8}{figure.9}}
\newlabel{fig:momentumsrate2}{{9}{8}{Visualization of error rates with various momentum values. Red and blue are cross-entropy error for test and train, while green and black are categorization error for test and train respectively}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Visualization of error rates with various momentum values. Red and blue are cross-entropy error for test and train, while green and black are categorization error for test and train respectively. }}{8}{figure.10}}
\newlabel{fig:momentumsrate3}{{10}{8}{Visualization of error rates with various momentum values. Red and blue are cross-entropy error for test and train, while green and black are categorization error for test and train respectively}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Visualization of VALIDATION error rates for the best two-layer network. Red and blue are cross-entropy error for test and validation, while green and black are categorization error for test and validation respectively. }}{8}{figure.11}}
\newlabel{fig:twolayerbest}{{11}{8}{Visualization of VALIDATION error rates for the best two-layer network. Red and blue are cross-entropy error for test and validation, while green and black are categorization error for test and validation respectively}{figure.11}{}}
